{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_previews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4PJYfgsAC7r0ZVm7VCk62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oldaandozerskaya/age_based_classification/blob/master/baseline_previews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKfc6KrPjWqb"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izAGFFj2dG_P"
      },
      "source": [
        "#Accumulated frequencies\n",
        "\n",
        "The code used for calcutating accumulated frequencies to find the most informative features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdspeH7l1tw-"
      },
      "source": [
        "#accumulated frequencies\n",
        "\n",
        "def acc_freq(mas):\n",
        "  masmax=max(mas)\n",
        "  masmin=min(mas)\n",
        "  n=10\n",
        "  part=(masmax-masmin)/float(n)\n",
        "  borders=np.zeros(n)\n",
        "  borders[0]=masmin+part\n",
        "  borders[1]=borders[0]+part\n",
        "  borders[2]=borders[1]+part\n",
        "  borders[3]=borders[2]+part\n",
        "  borders[4]=masmax\n",
        "\n",
        "  frequences=np.zeros(n)\n",
        "  for i in range(len(mas)):\n",
        "    for j, bord in enumerate(borders):\n",
        "      if mas[i]<=bord:\n",
        "        frequences[j]+=1\n",
        "        break\n",
        "\n",
        "  for i in range(len(frequences)):\n",
        "    frequences[i]=frequences[i]/float(len(mas))\n",
        "  \n",
        "  return frequences\n",
        "\n",
        "def acc(f1):\n",
        "  for i in range(1, len(f1)):\n",
        "    f1[i]=f1[i-1]+f1[i]\n",
        "  return f1\n",
        "\n",
        "import math\n",
        "\n",
        "def max_razn(f1, f2):\n",
        "  razn=[]\n",
        "  for i in range(len(f1)):\n",
        "    razn.append(math.fabs(f1[i]-f2[i]))\n",
        "  return max(razn)\n",
        "\n",
        "coefs=[]\n",
        "for j in range(additional_train.shape[1]):\n",
        "  mas1 = df.iloc[:,j].values\n",
        "  mas_ad=[]\n",
        "  mas_ch=[]\n",
        "  for z in range(len(train_labels)):\n",
        "    if train_labels[z]==0:\n",
        "      mas_ch.append(mas1[z])\n",
        "    else:\n",
        "      mas_ad.append(mas1[z])\n",
        "  freq1=acc(acc_freq(mas_ch))\n",
        "  freq2=acc(acc_freq(mas_ad))\n",
        "  print(feature_names[j])\n",
        "  print(freq1)\n",
        "  print(freq2)\n",
        "  coefs.append(max_razn(freq1, freq2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZKZzTsyCSGG"
      },
      "source": [
        "#print acc freq with feature names\n",
        "\n",
        "for i in range(len(coefs)):\n",
        "  print(str(coefs[i])+' - ' +str(feature_names[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITBpgMQUBWf_"
      },
      "source": [
        "#sorting frequencies\n",
        "nums_features = np.array(coefs).argsort()[-16:][::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ1RYq94BxYs"
      },
      "source": [
        "#top features\n",
        "\n",
        "for num in nums_features:\n",
        "  print(feature_names[num])\n",
        "  print(coefs[num])\n",
        "  mas1 = df.iloc[:,num].values\n",
        "  mas_ad=[]\n",
        "  mas_ch=[]\n",
        "  for z in range(len(train_labels)):\n",
        "    if train_labels[z]==0:\n",
        "      mas_ch.append(mas1[z])\n",
        "    else:\n",
        "      mas_ad.append(mas1[z])\n",
        "  print('children')\n",
        "  print(np.mean(np.array(mas_ch)))\n",
        "  print(np.std(np.array(mas_ch)))\n",
        "  print('adults')\n",
        "  print(np.mean(np.array(mas_ad)))\n",
        "  print(np.std(np.array(mas_ad)))\n",
        "  print('***')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1rJXzuBdfLF"
      },
      "source": [
        "#Baseline models \n",
        "\n",
        "The fragments of code related to the baseline models (RF & LSVC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0uVzNGD0Grd"
      },
      "source": [
        "#code for loading additional features\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/readability_train.csv', sep=';', dtype='unicode', index_col=0)\n",
        "additional_train=df.values[1:]\n",
        "print(additional_train.shape)\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/readability_test.csv', sep=';', index_col=0)\n",
        "additional_test=df.values[1:]\n",
        "print(additional_test.shape)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "additional_train = scaler.fit_transform(additional_train)\n",
        "additional_test = scaler.transform(additional_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-1e8lEjfjj"
      },
      "source": [
        "PATH_TRAIN = '/content/drive/My Drive/children_and_adults/train/'\n",
        "PATH_TEST = '/content/drive/My Drive/children_and_adults/test/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CACFisUAO3C4"
      },
      "source": [
        "#tfidf for abstracts\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "path_train='/content/drive/My Drive/children_and_adults/corpus/train'\n",
        "path_test='/content/drive/My Drive/children_and_adults/corpus/test'\n",
        "path_abstracts='/content/drive/My Drive/children_and_adults/corpus/abstracts'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqL3tBLJO45V"
      },
      "source": [
        "#adding abstracts\n",
        "\n",
        "files_previews=os.listdir(path_train)\n",
        "abstracts_train=[]\n",
        "\n",
        "for i,f in enumerate(files_previews):\n",
        "  with open(path_abstracts+'/'+f, 'r') as f42:\n",
        "    abstracts_train.append(f42.read())\n",
        "\n",
        "files_previews=os.listdir(path_test)\n",
        "abstracts_test=[]\n",
        "\n",
        "for i,f in enumerate(files_previews):\n",
        "  with open(path_abstracts+'/'+f, 'r') as f42:\n",
        "    abstracts_test.append(f42.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--eZFmnIO_sU"
      },
      "source": [
        "#adding abstracts\n",
        "\n",
        "!pip install pymorphy2\n",
        "import pymorphy2, re\n",
        "ma = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) #deleting newlines and line-breaks\n",
        "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text) #deleting symbols \n",
        "    text = \" \".join(ma.parse(word)[0].normal_form for word in text.split())\n",
        "    text = ' '.join(word for word in text.split() if len(word)>3)\n",
        "    return text\n",
        "\n",
        "abstracts_train_cleaned=[]\n",
        "abstracts_test_cleaned=[]\n",
        "\n",
        "for ab in abstracts_train:\n",
        "  abstracts_train_cleaned.append(clean_text(ab))\n",
        "for ab in abstracts_test:\n",
        "  abstracts_test_cleaned.append(clean_text(ab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPvX57Az5MV"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/train_texts_cleaned_short.pickle', 'rb') as handle:\n",
        "    train_texts=pickle.load(handle)\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/test_texts_cleaned_short.pickle', 'rb') as handle:\n",
        "    test_texts=pickle.load(handle)\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/train_labels2.pickle', 'rb') as handle:\n",
        "    train_labels=pickle.load(handle)\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/val_labels2.pickle', 'rb') as handle:\n",
        "    test_labels=pickle.load(handle)\n",
        "print(len(train_texts))\n",
        "print(len(test_texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGhNCpMOH9CM"
      },
      "source": [
        "#adding all features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/rutez_train.csv', sep=';', index_col=0)\n",
        "X=df.values\n",
        "print(X.shape)\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/readability_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/general_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/gram_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/lexical_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/age_rating_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "print(X.shape)\n",
        "df=pd.DataFrame(X)\n",
        "df.to_csv('all_features_train.csv', sep=';', header=0, index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo2RwYb52Tqy"
      },
      "source": [
        "df=pd.read_csv('all_features.csv', sep=';')\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPUxppfpkLG2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X = vectorizer.fit_transform(train_texts)\n",
        "X1 = vectorizer.transform(test_texts)\n",
        "#X = vectorizer.fit_transform(abstracts_train_cleaned)\n",
        "#X1 = vectorizer.transform(abstracts_test_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRUAbVv2JvZo"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZYStSDI8y8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X=np.hstack((X.todense(),additional_train))\n",
        "X1=np.hstack((X1.todense(),additional_test))\n",
        "#X=np.hstack((X,additional_train))\n",
        "#X1=np.hstack((X1,additional_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cBgPUTUKVxk"
      },
      "source": [
        "X=additional_train\n",
        "X1=additional_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5X5D_Xk-8B"
      },
      "source": [
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "#Random Forest\n",
        "best_res=0\n",
        "\n",
        "f1=0\n",
        "acc=0\n",
        "pres=0\n",
        "rec=0\n",
        "\n",
        "for rs in range(10):\n",
        "  RF=RandomForestClassifier(random_state=rs)\n",
        "  RF.fit(X, train_labels)\n",
        "  res=RF.predict(X1)\n",
        "  f1=f1_score(res, test_labels)*100\n",
        "  if f1>best_res:\n",
        "    best_res=f1\n",
        "    acc=accuracy_score(res, test_labels)*100\n",
        "    rec=recall_score(res, test_labels)*100\n",
        "    pres=precision_score(res, test_labels)*100\n",
        "\n",
        "print('Accuracy: %f' % acc)\n",
        "print('F1-score: %f' % best_res)\n",
        "print('Precision: %f' % pres)\n",
        "print('Recall: %f' % rec)\n",
        "\n",
        "#LInear SVC\n",
        "\n",
        "best_res=0\n",
        "\n",
        "f1=0\n",
        "acc=0\n",
        "pres=0\n",
        "rec=0\n",
        "\n",
        "for rs in range(10):\n",
        "  SVM=SVC(random_state=rs, tol=1e-5)\n",
        "  SVM.fit(X, train_labels)\n",
        "  res=SVM.predict(X1)\n",
        "  f1=f1_score(res, test_labels)*100\n",
        "  if f1>best_res:\n",
        "      best_res=f1\n",
        "      acc=accuracy_score(res, test_labels)*100\n",
        "      rec=recall_score(res, test_labels)*100\n",
        "      pres=precision_score(res, test_labels)*100\n",
        "\n",
        "print('Accuracy: %f' % acc)\n",
        "print('F1-score: %f' % best_res)\n",
        "print('Precision: %f' % pres)\n",
        "print('Recall: %f' % rec)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(.95)\n",
        "try:\n",
        "  X = pca.fit_transform(X.todense())\n",
        "  X1 = pca.transform(X1.todense())\n",
        "except:\n",
        "  X = pca.fit_transform(X)\n",
        "  X1 = pca.transform(X1) \n",
        "len(X[0])\n",
        "\n",
        "best_res=0\n",
        "\n",
        "f1=0\n",
        "acc=0\n",
        "pres=0\n",
        "rec=0\n",
        "\n",
        "for rs in range(10):\n",
        "  SVM=SVC(random_state=rs, tol=1e-5)\n",
        "  SVM.fit(X, train_labels)\n",
        "  res=SVM.predict(X1)\n",
        "  f1=f1_score(res, test_labels)*100\n",
        "  if f1>best_res:\n",
        "      best_res=f1\n",
        "      acc=accuracy_score(res, test_labels)*100\n",
        "      rec=recall_score(res, test_labels)*100\n",
        "      pres=precision_score(res, test_labels)*100\n",
        "\n",
        "print('Accuracy: %f' % acc)\n",
        "print('F1-score: %f' % best_res)\n",
        "print('Precision: %f' % pres)\n",
        "print('Recall: %f' % rec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn0IneEN9uz_"
      },
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(train_texts)\n",
        "X1 = vectorizer.transform(test_texts)\n",
        "\n",
        "#Random Forest\n",
        "\n",
        "RF=RandomForestClassifier(random_state=0)\n",
        "RF.fit(X, train_labels)\n",
        "res=RF.predict(X1)\n",
        "print('Accuracy: %f' % (accuracy_score(res, test_labels)*100))\n",
        "print('F1-score: %f' % (f1_score(res, test_labels)*100))\n",
        "print('Precision: %f' % (precision_score(res, test_labels)*100))\n",
        "print('Recall: %f' % (recall_score(res, test_labels)*100))\n",
        "\n",
        "#LInear SVC\n",
        "\n",
        "SVM=LinearSVC(random_state=0, tol=1e-5)\n",
        "SVM.fit(X, train_labels)\n",
        "res=SVM.predict(X1)\n",
        "print('Accuracy: %f' % (accuracy_score(res, test_labels)*100))\n",
        "print('F1-score: %f' % (f1_score(res, test_labels)*100))\n",
        "print('Precision: %f' % (precision_score(res, test_labels)*100))\n",
        "print('Recall: %f' % (recall_score(res, test_labels)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvyAeRC-mOAy"
      },
      "source": [
        "####readability\n",
        "\n",
        "import requests\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def average_sentence_length(text):\n",
        "  sentences=sent_tokenize(text)\n",
        "  l=0\n",
        "  for sent in sentences:\n",
        "    l+=len(word_tokenize(sent))\n",
        "  return l/len(sentences)\n",
        "\n",
        "def number_of_syllables(word):\n",
        "  count=0\n",
        "  word=word.lower()\n",
        "  for w in word:\n",
        "    if w=='а' or w=='ё' or w=='е' or w=='у' or w=='э' or w=='ю' or w=='и' or w=='я' or w=='ы' or w=='о':\n",
        "        count+=1\n",
        "  return count\n",
        "\n",
        "def average_syllabeles_word(text):\n",
        "  words=word_tokenize(text)\n",
        "  l=0\n",
        "  for word in words:\n",
        "    l+=number_of_syllables(word)\n",
        "  return l/len(words)\n",
        "\n",
        "values=[]\n",
        "values.append(['index_fk', 'index_cl', 'index_dc', 'index_ari', 'index_SMOG'])\n",
        "\n",
        "for text in train_texts:\n",
        "  response = requests.post(\"http://api.plainrussian.ru/api/1.0/ru/measure/\", data={\"text\":text})\n",
        "  res=response.json()\n",
        "  values.append([res['indexes']['index_fk'], res['indexes']['index_cl'], res['indexes']['index_dc'], res['indexes']['index_ari'], res['indexes']['index_SMOG']])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxC2I7YQ3Bzj"
      },
      "source": [
        "df=pd.DataFrame(values)\n",
        "df[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R675Z7smns0l"
      },
      "source": [
        "df.to_csv('readability_train.csv', sep=';', inde)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdWXlLtM_cH"
      },
      "source": [
        "with open('/content/drive/My Drive/children_and_adults/5000lemma.al', 'r', encoding='cp1251') as handle:\n",
        "  content=handle.readlines()\n",
        "\n",
        "words=[]\n",
        "freq=[]\n",
        "for i in range(1,len(content)):\n",
        "  elems=content[i].split()\n",
        "  words.append(elems[2])\n",
        "  freq.append(elems[1])\n",
        "\n",
        "dictionary = dict(zip(words, freq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc65REdVT7GG"
      },
      "source": [
        "path_freq='/content/drive/My Drive/freqrnc2011.csv'\n",
        "df=pd.read_csv(path_freq, sep='\\t')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3POrPInWLkR"
      },
      "source": [
        "list_dict=df['Lemma'].values\n",
        "pos_dict=df['PoS'].values\n",
        "freq_dict=df['Freq(ipm)'].values\n",
        "r_dict=df['R'].values\n",
        "d_dict=df['D'].values\n",
        "doc_dict=df['Doc'].values\n",
        "\n",
        "values_lex=[]\n",
        "values_lex.append(['words_fr', 'words_r', 'words_d', 'words_doc', 'v_fr', 'v_r', 'v_d', 'v_doc',\\\n",
        "            's_fr', 's_r', 's_d', 's_doc', 'adv_fr', 'adv_r', 'adv_d', 'adv_doc',\\\n",
        "            'adj_fr', 'adj_r', 'adj_d', 'adj_doc', 'prop_fr', 'prop_r', 'prop_d', 'prop_doc', '5000_proc', '5000_freq' ])\n",
        "values_gram=[]\n",
        "values_gram.append(['count_v', 'count_s', 'count_adj'])\n",
        "\n",
        "list_dict=[w.lower() for w in list_dict]\n",
        "\n",
        "for t in train_texts:\n",
        "  words=t.split()\n",
        "\n",
        "  words_in_list=0\n",
        "  verbs_in_list=0\n",
        "  nouns_in_list=0\n",
        "  adv_in_list=0\n",
        "  adj_in_list=0\n",
        "  propn_in_list=0\n",
        "\n",
        "  words_freq=0\n",
        "  words_r=0\n",
        "  words_d=0\n",
        "  words_doc=0\n",
        "\n",
        "  v_freq=0\n",
        "  v_r=0\n",
        "  v_d=0\n",
        "  v_doc=0\n",
        "\n",
        "  adv_freq=0\n",
        "  adv_r=0\n",
        "  adv_d=0\n",
        "  adv_doc=0\n",
        "\n",
        "  s_freq=0\n",
        "  s_r=0\n",
        "  s_d=0\n",
        "  s_doc=0\n",
        "\n",
        "  a_freq=0\n",
        "  a_r=0\n",
        "  a_d=0\n",
        "  a_doc=0\n",
        "\n",
        "  p_freq=0\n",
        "  p_r=0\n",
        "  p_d=0\n",
        "  p_doc=0\n",
        "\n",
        "  number_words=len(words)\n",
        "  number_in_list=0\n",
        "  number_out_of_list=0\n",
        "  freq=0\n",
        "\n",
        "  for word in words:\n",
        "    try:\n",
        "      freq=dictionary[word]\n",
        "      number_in_list+=1\n",
        "    except:\n",
        "      number_out_of_list+=1\n",
        "\n",
        "    for i, item in enumerate(list_dict):\n",
        "      if word==item:\n",
        "        words_in_list+=1\n",
        "        words_freq+=freq_dict[i]\n",
        "        words_r+=r_dict[i]\n",
        "        words_d+=d_dict[i]\n",
        "        words_doc+=doc_dict[i]\n",
        "\n",
        "        if pos_dict[i]=='v':\n",
        "          verbs_in_list+=1\n",
        "          v_freq+=freq_dict[i]\n",
        "          v_r+=r_dict[i]\n",
        "          v_d+=d_dict[i]\n",
        "          v_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='adv' or pos_dict[i]=='advpro':\n",
        "          adv_in_list+=1\n",
        "          adv_freq+=freq_dict[i]\n",
        "          adv_r+=r_dict[i]\n",
        "          adv_d+=d_dict[i]\n",
        "          adv_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s':\n",
        "          nouns_in_list+=1\n",
        "          s_freq+=freq_dict[i]\n",
        "          s_r+=r_dict[i]\n",
        "          s_d+=d_dict[i]\n",
        "          s_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='a':\n",
        "          adj_in_list+=1\n",
        "          a_freq+=freq_dict[i]\n",
        "          a_r+=r_dict[i]\n",
        "          a_d+=d_dict[i]\n",
        "          a_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s.PROP':\n",
        "          propn_in_list+=1\n",
        "          p_freq+=freq_dict[i]\n",
        "          p_r+=r_dict[i]\n",
        "          p_d+=d_dict[i]\n",
        "          p_doc+=doc_dict[i]\n",
        "\n",
        "  new_line=[]\n",
        "  if words_in_list!=0:\n",
        "    new_line.append(words_freq/words_in_list)\n",
        "    new_line.append(words_r/words_in_list)\n",
        "    new_line.append(words_d/words_in_list)\n",
        "    new_line.append(words_doc/words_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if verbs_in_list!=0:\n",
        "    new_line.append(v_freq/verbs_in_list)\n",
        "    new_line.append(v_r/verbs_in_list)\n",
        "    new_line.append(v_d/verbs_in_list)\n",
        "    new_line.append(v_doc/verbs_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if nouns_in_list!=0:\n",
        "    new_line.append(s_freq/nouns_in_list)\n",
        "    new_line.append(s_r/nouns_in_list)\n",
        "    new_line.append(s_d/nouns_in_list)\n",
        "    new_line.append(s_doc/nouns_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if adv_in_list!=0:\n",
        "    new_line.append(adv_freq/adv_in_list)\n",
        "    new_line.append(adv_r/adv_in_list)\n",
        "    new_line.append(adv_d/adv_in_list)\n",
        "    new_line.append(adv_doc/adv_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if adj_in_list!=0:\n",
        "    new_line.append(a_freq/adj_in_list)\n",
        "    new_line.append(a_r/adj_in_list)\n",
        "    new_line.append(a_d/adj_in_list)\n",
        "    new_line.append(a_doc/adj_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if propn_in_list!=0:\n",
        "    new_line.append(p_freq/propn_in_list)\n",
        "    new_line.append(p_r/propn_in_list)\n",
        "    new_line.append(p_d/propn_in_list)\n",
        "    new_line.append(p_doc/propn_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_words!=0:\n",
        "    new_line.append(float(number_in_list)/number_words)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_in_list!=0:\n",
        "    new_line.append(float(freq)/number_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "\n",
        "  values_lex.append(new_line)\n",
        "  new_line=[float(verbs_in_list)/len(words), float(nouns_in_list)/len(words), float(adj_in_list)/len(words)]\n",
        "  values_gram.append(new_line)\n",
        "\n",
        "df=pd.DataFrame(values_lex)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/lexical_train.csv', sep=';')\n",
        "df=pd.DataFrame(values_gram)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/gram_train.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqPzEvNen0Y0"
      },
      "source": [
        "list_dict=df['Lemma'].values\n",
        "pos_dict=df['PoS'].values\n",
        "freq_dict=df['Freq(ipm)'].values\n",
        "r_dict=df['R'].values\n",
        "d_dict=df['D'].values\n",
        "doc_dict=df['Doc'].values\n",
        "\n",
        "values_lex=[]\n",
        "values_lex.append(['words_fr', 'words_r', 'words_d', 'words_doc', 'v_fr', 'v_r', 'v_d', 'v_doc',\\\n",
        "            's_fr', 's_r', 's_d', 's_doc', 'adv_fr', 'adv_r', 'adv_d', 'adv_doc',\\\n",
        "            'adj_fr', 'adj_r', 'adj_d', 'adj_doc', 'prop_fr', 'prop_r', 'prop_d', 'prop_doc', '5000_proc', '5000_freq' ])\n",
        "values_gram=[]\n",
        "values_gram.append(['count_v', 'count_s', 'count_adj'])\n",
        "\n",
        "list_dict=[w.lower() for w in list_dict]\n",
        "\n",
        "for t in test_texts:\n",
        "  words=t.split()\n",
        "\n",
        "  words_in_list=0\n",
        "  verbs_in_list=0\n",
        "  nouns_in_list=0\n",
        "  adv_in_list=0\n",
        "  adj_in_list=0\n",
        "  propn_in_list=0\n",
        "\n",
        "  words_freq=0\n",
        "  words_r=0\n",
        "  words_d=0\n",
        "  words_doc=0\n",
        "\n",
        "  v_freq=0\n",
        "  v_r=0\n",
        "  v_d=0\n",
        "  v_doc=0\n",
        "\n",
        "  adv_freq=0\n",
        "  adv_r=0\n",
        "  adv_d=0\n",
        "  adv_doc=0\n",
        "\n",
        "  s_freq=0\n",
        "  s_r=0\n",
        "  s_d=0\n",
        "  s_doc=0\n",
        "\n",
        "  a_freq=0\n",
        "  a_r=0\n",
        "  a_d=0\n",
        "  a_doc=0\n",
        "\n",
        "  p_freq=0\n",
        "  p_r=0\n",
        "  p_d=0\n",
        "  p_doc=0\n",
        "\n",
        "  number_words=len(words)\n",
        "  number_in_list=0\n",
        "  number_out_of_list=0\n",
        "  freq=0\n",
        "\n",
        "  for word in words:\n",
        "    try:\n",
        "      freq=dictionary[word]\n",
        "      number_in_list+=1\n",
        "    except:\n",
        "      number_out_of_list+=1\n",
        "\n",
        "    for i, item in enumerate(list_dict):\n",
        "      if word==item:\n",
        "        words_in_list+=1\n",
        "        words_freq+=freq_dict[i]\n",
        "        words_r+=r_dict[i]\n",
        "        words_d+=d_dict[i]\n",
        "        words_doc+=doc_dict[i]\n",
        "\n",
        "        if pos_dict[i]=='v':\n",
        "          verbs_in_list+=1\n",
        "          v_freq+=freq_dict[i]\n",
        "          v_r+=r_dict[i]\n",
        "          v_d+=d_dict[i]\n",
        "          v_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='adv' or pos_dict[i]=='advpro':\n",
        "          adv_in_list+=1\n",
        "          adv_freq+=freq_dict[i]\n",
        "          adv_r+=r_dict[i]\n",
        "          adv_d+=d_dict[i]\n",
        "          adv_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s':\n",
        "          nouns_in_list+=1\n",
        "          s_freq+=freq_dict[i]\n",
        "          s_r+=r_dict[i]\n",
        "          s_d+=d_dict[i]\n",
        "          s_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='a':\n",
        "          adj_in_list+=1\n",
        "          a_freq+=freq_dict[i]\n",
        "          a_r+=r_dict[i]\n",
        "          a_d+=d_dict[i]\n",
        "          a_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s.PROP':\n",
        "          propn_in_list+=1\n",
        "          p_freq+=freq_dict[i]\n",
        "          p_r+=r_dict[i]\n",
        "          p_d+=d_dict[i]\n",
        "          p_doc+=doc_dict[i]\n",
        "\n",
        "  new_line=[]\n",
        "  if words_in_list!=0:\n",
        "    new_line.append(words_freq/words_in_list)\n",
        "    new_line.append(words_r/words_in_list)\n",
        "    new_line.append(words_d/words_in_list)\n",
        "    new_line.append(words_doc/words_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if verbs_in_list!=0:\n",
        "    new_line.append(v_freq/verbs_in_list)\n",
        "    new_line.append(v_r/verbs_in_list)\n",
        "    new_line.append(v_d/verbs_in_list)\n",
        "    new_line.append(v_doc/verbs_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if nouns_in_list!=0:\n",
        "    new_line.append(s_freq/nouns_in_list)\n",
        "    new_line.append(s_r/nouns_in_list)\n",
        "    new_line.append(s_d/nouns_in_list)\n",
        "    new_line.append(s_doc/nouns_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if adv_in_list!=0:\n",
        "    new_line.append(adv_freq/adv_in_list)\n",
        "    new_line.append(adv_r/adv_in_list)\n",
        "    new_line.append(adv_d/adv_in_list)\n",
        "    new_line.append(adv_doc/adv_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if adj_in_list!=0:\n",
        "    new_line.append(a_freq/adj_in_list)\n",
        "    new_line.append(a_r/adj_in_list)\n",
        "    new_line.append(a_d/adj_in_list)\n",
        "    new_line.append(a_doc/adj_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if propn_in_list!=0:\n",
        "    new_line.append(p_freq/propn_in_list)\n",
        "    new_line.append(p_r/propn_in_list)\n",
        "    new_line.append(p_d/propn_in_list)\n",
        "    new_line.append(p_doc/propn_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_words!=0:\n",
        "    new_line.append(float(number_in_list)/number_words)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_in_list!=0:\n",
        "    new_line.append(float(freq)/number_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "\n",
        "  values_lex.append(new_line)\n",
        "  new_line=[float(verbs_in_list)/len(words), float(nouns_in_list)/len(words), float(adj_in_list)/len(words)]\n",
        "  values_gram.append(new_line)\n",
        "\n",
        "df=pd.DataFrame(values_lex)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/lexical_test.csv', sep=';')\n",
        "df=pd.DataFrame(values_gram)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/gram_test.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTFJlFGiVL37"
      },
      "source": [
        "for i,p in enumerate(df['PoS'].values):\n",
        "  if p=='pr':\n",
        "    print(df['Lemma'].values[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDRu_9RFF5V9"
      },
      "source": [
        "###ЧАСТОТНЫЙ СЛОВАРЬ\n",
        "\n",
        "values = []\n",
        "values.append([])\n",
        "\n",
        "for t in test_texts:\n",
        "  text_words=t.split()\n",
        "  number_words=len(text_words)\n",
        "  number_in_list=0\n",
        "  number_out_of_list=0\n",
        "  freq=0\n",
        "  for w in text_words:\n",
        "    try:\n",
        "      freq=dictionary[w]\n",
        "      number_in_list+=1\n",
        "    except:\n",
        "      number_out_of_list+=1\n",
        "\n",
        "  try:\n",
        "    values.append([float(number_in_list)/number_words, float(freq)/number_in_list])\n",
        "  except:\n",
        "    values.append([0, 0])\n",
        "df=pd.DataFrame(values)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/lexmin_test.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT7z2lNFF3NH"
      },
      "source": [
        "#age rating\n",
        "import os\n",
        "\n",
        "PATH_DESCRIPTIONS = '/content/drive/My Drive/children_and_adults/description.csv'\n",
        "df=pd.read_csv(PATH_DESCRIPTIONS, sep=';', header=None)\n",
        "\n",
        "dictionary = dict(zip(df[0], df[3]))\n",
        "\n",
        "PATH_TRAIN = '/content/drive/My Drive/children_and_adults/train/'\n",
        "PATH_TEST = '/content/drive/My Drive/children_and_adults/test/'\n",
        "\n",
        "files=os.listdir(PATH_TRAIN)\n",
        "print(len(files))\n",
        "ratings=[]\n",
        "ratings.append(['age_rating'])\n",
        "\n",
        "for f in files:\n",
        "  rating=dictionary[str(f)]\n",
        "  if rating==0:\n",
        "    rating.append([0])\n",
        "  elif rating==6:\n",
        "    ratings.append([0.25])\n",
        "  elif rating==12:\n",
        "    ratings.append([0.5])\n",
        "  elif rating==16:\n",
        "    ratings.append([0.75])\n",
        "  elif rating==18:\n",
        "    ratings.append([1])\n",
        "  else:\n",
        "    ratings.append([0.5])\n",
        "df=pd.DataFrame(ratings)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/age_rating_train.csv', sep=';')\n",
        "\n",
        "files=os.listdir(PATH_TEST)\n",
        "ratings=[]\n",
        "ratings.append(['age_rating'])\n",
        "\n",
        "for f in files:\n",
        "  rating=dictionary[str(f)]\n",
        "  if rating==0:\n",
        "    rating.append([0])\n",
        "  elif rating==6:\n",
        "    ratings.append([0.25])\n",
        "  elif rating==12:\n",
        "    ratings.append([0.5])\n",
        "  elif rating==16:\n",
        "    ratings.append([0.75])\n",
        "  elif rating==18:\n",
        "    ratings.append([1])\n",
        "  else:\n",
        "    ratings.append([0.5])\n",
        "df=pd.DataFrame(ratings)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/age_rating_test.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}