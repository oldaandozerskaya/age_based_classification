{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_previews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4PJYfgsAC7r0ZVm7VCk62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oldaandozerskaya/age_based_classification/blob/master/baseline_previews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKfc6KrPjWqb",
        "outputId": "30637129-f94b-46ea-a888-f8b4f0e3ab9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izAGFFj2dG_P"
      },
      "source": [
        "#Accumulated frequencies\n",
        "\n",
        "The code used for calcutating accumulated frequencies to find the most informative features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdspeH7l1tw-",
        "outputId": "340abb7a-0a6f-43f1-f119-3d8f2e2035ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#accumulated frequencies\n",
        "\n",
        "def acc_freq(mas):\n",
        "  masmax=max(mas)\n",
        "  masmin=min(mas)\n",
        "  n=10\n",
        "  part=(masmax-masmin)/float(n)\n",
        "  borders=np.zeros(n)\n",
        "  borders[0]=masmin+part\n",
        "  borders[1]=borders[0]+part\n",
        "  borders[2]=borders[1]+part\n",
        "  borders[3]=borders[2]+part\n",
        "  borders[4]=masmax\n",
        "\n",
        "  frequences=np.zeros(n)\n",
        "  for i in range(len(mas)):\n",
        "    for j, bord in enumerate(borders):\n",
        "      if mas[i]<=bord:\n",
        "        frequences[j]+=1\n",
        "        break\n",
        "\n",
        "  for i in range(len(frequences)):\n",
        "    frequences[i]=frequences[i]/float(len(mas))\n",
        "  \n",
        "  return frequences\n",
        "\n",
        "def acc(f1):\n",
        "  for i in range(1, len(f1)):\n",
        "    f1[i]=f1[i-1]+f1[i]\n",
        "  return f1\n",
        "\n",
        "import math\n",
        "\n",
        "def max_razn(f1, f2):\n",
        "  razn=[]\n",
        "  for i in range(len(f1)):\n",
        "    razn.append(math.fabs(f1[i]-f2[i]))\n",
        "  return max(razn)\n",
        "\n",
        "coefs=[]\n",
        "for j in range(additional_train.shape[1]):\n",
        "  mas1 = df.iloc[:,j].values\n",
        "  mas_ad=[]\n",
        "  mas_ch=[]\n",
        "  for z in range(len(train_labels)):\n",
        "    if train_labels[z]==0:\n",
        "      mas_ch.append(mas1[z])\n",
        "    else:\n",
        "      mas_ad.append(mas1[z])\n",
        "  freq1=acc(acc_freq(mas_ch))\n",
        "  freq2=acc(acc_freq(mas_ad))\n",
        "  print(feature_names[j])\n",
        "  print(freq1)\n",
        "  print(freq2)\n",
        "  coefs.append(max_razn(freq1, freq2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_fact\n",
            "[0.37855787 0.63519924 0.81593928 0.92552182 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.34605705 0.61283557 0.78984899 0.89932886 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "pos_opinion\n",
            "[0.06214421 0.32779886 0.63425047 0.84440228 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.05369128 0.24119128 0.53817114 0.77432886 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "pos_feeling\n",
            "[0.36242884 0.69165085 0.86242884 0.94639469 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.34731544 0.55411074 0.70721477 0.84144295 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "neg_fact\n",
            "[0.38614801 0.65132827 0.80550285 0.90844402 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.29110738 0.60025168 0.80327181 0.91526846 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "neg_opinion\n",
            "[0.09535104 0.32779886 0.60294118 0.79506641 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.09941275 0.32088926 0.59186242 0.79781879 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "neg_feeling\n",
            "[0.45920304 0.67504744 0.82495256 0.90749526 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.49077181 0.80662752 0.9341443  0.97860738 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "index_fk\n",
            "[0.02182163 0.19259962 0.53842505 0.84535104 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.07969799 0.66694631 0.97147651 0.99790268 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "index_cl\n",
            "[4.74383302e-04 9.48766603e-04 9.48766603e-04 9.48766603e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[4.19463087e-04 4.19463087e-04 4.19463087e-04 4.19463087e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "index_dc\n",
            "[0.00237192 0.13092979 0.49905123 0.82685009 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.0977349  0.7852349  0.98951342 0.99916107 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "index_ari\n",
            "[0.0142315  0.12286528 0.42125237 0.77514231 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.09228188 0.64932886 0.96350671 0.99874161 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "index_SMOG\n",
            "[0.0056926  0.05882353 0.3097723  0.62191651 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00377517 0.09060403 0.43666107 0.78565436 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "avg_words_len\n",
            "[4.74383302e-04 1.89753321e-03 9.48766603e-03 7.21062619e-02\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[0.00209732 0.02516779 0.16107383 0.41862416 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "avg_sent_len\n",
            "[0.17694497 0.65844402 0.91793169 0.98339658 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.9840604  0.99832215 0.99958054 0.99958054 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "med_sent_len\n",
            "[0.22485769 0.63709677 0.88187856 0.97058824 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.97063758 0.99832215 0.99916107 0.99916107 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "med_words_len\n",
            "[0.00474383 0.00474383 0.00474383 0.54364326 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[8.38926174e-04 8.38926174e-04 8.38926174e-04 3.41023490e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "avg_count_syl\n",
            "[0.00142315 0.00189753 0.00332068 0.00521822 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[8.38926174e-04 8.38926174e-04 8.38926174e-04 8.38926174e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "many_syllables\n",
            "[0.10673624 0.38282732 0.65986717 0.82637571 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.05285235 0.33263423 0.65897651 0.82508389 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "len_text\n",
            "[0.00332068 0.01091082 0.02039848 0.02893738 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[4.19463087e-04 8.38926174e-04 8.38926174e-04 1.67785235e-03\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "ttr\n",
            "[9.48766603e-04 6.16698292e-03 1.75521822e-02 5.64516129e-02\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[0.0033557  0.00629195 0.0113255  0.01845638 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "ttr_a\n",
            "[9.48766603e-04 9.48766603e-04 9.48766603e-04 2.84629981e-03\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[0.00209732 0.00755034 0.02348993 0.06627517 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "ttr_n\n",
            "[4.74383302e-04 4.74383302e-04 4.74383302e-04 2.37191651e-03\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[0.0033557  0.01048658 0.0385906  0.10444631 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "ttr_v\n",
            "[0.65749526 0.65986717 0.66318786 0.66888046 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.64303691 0.64387584 0.64597315 0.65268456 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "nav\n",
            "[0.95777989 0.99051233 0.99667932 0.99762808 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.64303691 0.95721477 0.97860738 0.99119128 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "count_v\n",
            "[0.00806452 0.01328273 0.0313093  0.15607211 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.01677852 0.02097315 0.03984899 0.13506711 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "count_s\n",
            "[0.00189753 0.0056926  0.01091082 0.02087287 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00880872 0.01468121 0.01552013 0.01719799 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "count_adj\n",
            "[0.0085389  0.01802657 0.07210626 0.23671727 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.01342282 0.0192953  0.06753356 0.26761745 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "words_fr\n",
            "[0.02609108 0.60104364 0.96157495 0.99762808 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.02600671 0.50293624 0.92533557 0.99286913 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "words_r\n",
            "[4.74383302e-04 4.74383302e-04 4.74383302e-04 4.74383302e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[8.38926174e-04 2.93624161e-03 4.61409396e-03 5.45302013e-03\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "words_d\n",
            "[4.74383302e-04 4.74383302e-04 4.74383302e-04 4.74383302e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[4.19463087e-04 8.38926174e-04 8.38926174e-04 2.51677852e-03\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "words_doc\n",
            "[4.74383302e-04 9.48766603e-04 1.09108159e-02 1.60815939e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[0.00125839 0.00293624 0.03313758 0.24496644 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "v_fr\n",
            "[0.45540797 0.90085389 0.99003795 0.99620493 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.70973154 0.97483221 0.99412752 0.99580537 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "v_r\n",
            "[0.00142315 0.00142315 0.00142315 0.00142315 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00209732 0.00209732 0.00209732 0.00251678 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "v_d\n",
            "[0.00142315 0.00142315 0.00142315 0.00142315 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00167785 0.00209732 0.00209732 0.00209732 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "v_doc\n",
            "[0.00759013 0.2471537  0.7727704  0.96821632 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.05075503 0.66065436 0.96686242 0.99454698 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "s_fr\n",
            "[0.00521822 0.19165085 0.69402277 0.92552182 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00503356 0.13548658 0.59941275 0.8989094  1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "s_r\n",
            "[9.48766603e-04 9.48766603e-04 9.48766603e-04 9.48766603e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[8.38926174e-04 8.38926174e-04 8.38926174e-04 8.38926174e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "s_d\n",
            "[9.48766603e-04 9.48766603e-04 9.48766603e-04 9.48766603e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[8.38926174e-04 8.38926174e-04 8.38926174e-04 8.38926174e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "s_doc\n",
            "[9.48766603e-04 2.84629981e-03 2.51423150e-02 1.68880455e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "[0.00251678 0.01048658 0.18246644 0.65142617 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adv_fr\n",
            "[0.00711575 0.03842505 0.18074004 0.45398482 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.02097315 0.16862416 0.53649329 0.85738255 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adv_r\n",
            "[0.00189753 0.00189753 0.00189753 0.00189753 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00671141 0.00671141 0.00671141 0.0079698  1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adv_d\n",
            "[0.00189753 0.00189753 0.00189753 0.00189753 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00671141 0.00671141 0.00671141 0.00671141 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adv_doc\n",
            "[0.00426945 0.01043643 0.0683112  0.28083491 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.01090604 0.01677852 0.07676174 0.2932047  1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adj_fr\n",
            "[0.01518027 0.23197343 0.59013283 0.84203036 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.11870805 0.75083893 0.96812081 0.99580537 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adj_r\n",
            "[0.00142315 0.00142315 0.00142315 0.00142315 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00125839 0.00125839 0.00125839 0.00125839 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adj_d\n",
            "[0.00142315 0.00142315 0.00142315 0.00142315 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00125839 0.00125839 0.00125839 0.00125839 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "adj_doc\n",
            "[0.0028463  0.04459203 0.28605313 0.63330171 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.01090604 0.32172819 0.83053691 0.98028523 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "prop_fr\n",
            "[0.61812144 0.86717268 0.9516129  0.98055028 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.73196309 0.93624161 0.98112416 0.99328859 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "prop_r\n",
            "[0.03510436 0.03510436 0.03510436 0.03557875 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.03607383 0.03607383 0.03649329 0.03691275 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "prop_d\n",
            "[0.03510436 0.03510436 0.03510436 0.03510436 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.03607383 0.03607383 0.03607383 0.03607383 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "prop_doc\n",
            "[0.53605313 0.80834915 0.90986717 0.96204934 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.62332215 0.88506711 0.96644295 0.98825503 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "5000_proc\n",
            "[0.00189753 0.00996205 0.01280835 0.01375712 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.01258389 0.0159396  0.0159396  0.01635906 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "5000_freq\n",
            "[0.9573055  0.97533207 0.99762808 0.99952562 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.9966443  0.99748322 0.99874161 0.99874161 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "age_rating\n",
            "[0.44734345 0.44734345 0.44734345 0.90370019 1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "[0.00503356 0.00503356 0.00503356 0.15855705 1.         1.\n",
            " 1.         1.         1.         1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZKZzTsyCSGG",
        "outputId": "efe3ecbd-6e92-4d03-e166-65d5c4fb6973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "source": [
        "#print acc freq with feature names\n",
        "\n",
        "for i in range(len(coefs)):\n",
        "  print(str(coefs[i])+' - ' +str(feature_names[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.032500827782942554 - pos_fact\n",
            "0.09607933344370445 - pos_opinion\n",
            "0.15521407740407256 - pos_feeling\n",
            "0.09504062503979727 - neg_fact\n",
            "0.011078760363205764 - neg_opinion\n",
            "0.13158007844835273 - neg_feeling\n",
            "0.4743466882314735 - index_fk\n",
            "0.0005293035161672377 - index_cl\n",
            "0.6543051080575117 - index_dc\n",
            "0.5422543394928875 - index_ari\n",
            "0.16373785387720796 - index_SMOG\n",
            "0.3465178992142429 - avg_words_len\n",
            "0.8071154311475619 - avg_sent_len\n",
            "0.7457798988831298 - med_sent_len\n",
            "0.2026197738242299 - med_words_len\n",
            "0.004379290144288935 - avg_count_syl\n",
            "0.05388389389096188 - many_syllables\n",
            "0.027259529055181284 - len_text\n",
            "0.037995237064299636 - ttr\n",
            "0.06342886797498822 - ttr_a\n",
            "0.10207439221629332 - ttr_n\n",
            "0.017214701425060097 - ttr_v\n",
            "0.31474297339632973 - nav\n",
            "0.02100499216789986 - count_v\n",
            "0.008988608433197917 - count_s\n",
            "0.03090018211224735 - count_adj\n",
            "0.0981074016530189 - words_fr\n",
            "0.004978636832520408 - words_r\n",
            "0.0020423952217821528 - words_d\n",
            "0.08415050367408275 - words_doc\n",
            "0.25432357398469235 - v_fr\n",
            "0.001093628618366593 - v_r\n",
            "0.0006741655311182711 - v_d\n",
            "0.41350066222635407 - v_doc\n",
            "0.09461001872062957 - s_fr\n",
            "0.00010984042891891551 - s_r\n",
            "0.00010984042891891551 - s_d\n",
            "0.48254571908867466 - s_doc\n",
            "0.4033977306012252 - adv_fr\n",
            "0.006072265450887 - adv_r\n",
            "0.004813876189142034 - adv_d\n",
            "0.012369783375571486 - adv_doc\n",
            "0.5188654916393923 - adj_fr\n",
            "0.00016476064337837327 - adj_r\n",
            "0.00016476064337837327 - adj_d\n",
            "0.5444837818218864 - adj_doc\n",
            "0.11384164512308492 - prop_fr\n",
            "0.0013889242642283195 - prop_r\n",
            "0.000969461176979998 - prop_d\n",
            "0.08726901672121545 - prop_doc\n",
            "0.010686359410618544 - 5000_proc\n",
            "0.03933879245571359 - 5000_freq\n",
            "0.745143142773455 - age_rating\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITBpgMQUBWf_"
      },
      "source": [
        "#sorting frequencies\n",
        "nums_features = np.array(coefs).argsort()[-16:][::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ1RYq94BxYs",
        "outputId": "d9cfb219-7d4f-41d4-b5f6-1ba9ff96c513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#top features\n",
        "\n",
        "for num in nums_features:\n",
        "  print(feature_names[num])\n",
        "  print(coefs[num])\n",
        "  mas1 = df.iloc[:,num].values\n",
        "  mas_ad=[]\n",
        "  mas_ch=[]\n",
        "  for z in range(len(train_labels)):\n",
        "    if train_labels[z]==0:\n",
        "      mas_ch.append(mas1[z])\n",
        "    else:\n",
        "      mas_ad.append(mas1[z])\n",
        "  print('children')\n",
        "  print(np.mean(np.array(mas_ch)))\n",
        "  print(np.std(np.array(mas_ch)))\n",
        "  print('adults')\n",
        "  print(np.mean(np.array(mas_ad)))\n",
        "  print(np.std(np.array(mas_ad)))\n",
        "  print('***')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_sent_len\n",
            "0.8071154311475619\n",
            "children\n",
            "88.68530310670343\n",
            "30.638899923751378\n",
            "adults\n",
            "105.65092738105058\n",
            "54.507231574487754\n",
            "***\n",
            "med_sent_len\n",
            "0.7457798988831298\n",
            "children\n",
            "79.69449715370018\n",
            "34.004933620221145\n",
            "adults\n",
            "97.89974832214764\n",
            "59.062222052359\n",
            "***\n",
            "age_rating\n",
            "0.745143142773455\n",
            "children\n",
            "0.42101518026565465\n",
            "0.18525973806401688\n",
            "adults\n",
            "0.7686661073825504\n",
            "0.15936161211235558\n",
            "***\n",
            "index_dc\n",
            "0.6543051080575117\n",
            "children\n",
            "6.363805854482636\n",
            "2.085423930094967\n",
            "adults\n",
            "7.847778830722816\n",
            "2.6420999831227094\n",
            "***\n",
            "adj_doc\n",
            "0.5444837818218864\n",
            "children\n",
            "3669.481657844042\n",
            "1173.7798674200717\n",
            "adults\n",
            "3484.275850841236\n",
            "1074.0252236907902\n",
            "***\n",
            "index_ari\n",
            "0.5422543394928875\n",
            "children\n",
            "7.399919628948038\n",
            "2.9290302741034004\n",
            "adults\n",
            "9.286668183616912\n",
            "3.5275394855184707\n",
            "***\n",
            "adj_fr\n",
            "0.5188654916393923\n",
            "children\n",
            "146.96398722474814\n",
            "60.39003986592684\n",
            "adults\n",
            "135.29837683636816\n",
            "53.56262300408716\n",
            "***\n",
            "s_doc\n",
            "0.48254571908867466\n",
            "children\n",
            "3370.5182632926376\n",
            "766.392226526394\n",
            "adults\n",
            "3705.731557489055\n",
            "838.4539166538256\n",
            "***\n",
            "index_fk\n",
            "0.4743466882314735\n",
            "children\n",
            "7.360268799175548\n",
            "2.9511834313722725\n",
            "adults\n",
            "9.208224804978375\n",
            "3.662529059981521\n",
            "***\n",
            "v_doc\n",
            "0.41350066222635407\n",
            "children\n",
            "6481.526616881508\n",
            "1937.8028729698553\n",
            "adults\n",
            "6255.703182574523\n",
            "2243.7601435587844\n",
            "***\n",
            "adv_fr\n",
            "0.4033977306012252\n",
            "children\n",
            "277.43298086690874\n",
            "90.92860035201376\n",
            "adults\n",
            "239.17302074404074\n",
            "84.2640569144753\n",
            "***\n",
            "avg_words_len\n",
            "0.3465178992142429\n",
            "children\n",
            "6.80465315040834\n",
            "0.29730040143417835\n",
            "adults\n",
            "7.025061752367739\n",
            "0.3289332668700086\n",
            "***\n",
            "nav\n",
            "0.31474297339632973\n",
            "children\n",
            "0.5878165681139004\n",
            "1.0389520253153557\n",
            "adults\n",
            "0.6245732374136365\n",
            "0.9410742129640328\n",
            "***\n",
            "v_fr\n",
            "0.25432357398469235\n",
            "children\n",
            "1073.5918873899932\n",
            "605.4037731691501\n",
            "adults\n",
            "1002.9407296830954\n",
            "740.1653883194045\n",
            "***\n",
            "med_words_len\n",
            "0.2026197738242299\n",
            "children\n",
            "6.4539848197343455\n",
            "0.5119708021326536\n",
            "adults\n",
            "6.671140939597316\n",
            "0.49839275932956456\n",
            "***\n",
            "index_SMOG\n",
            "0.16373785387720796\n",
            "children\n",
            "6.994348561523548\n",
            "2.263646983832634\n",
            "adults\n",
            "8.638463595516932\n",
            "2.6774158446772067\n",
            "***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1rJXzuBdfLF"
      },
      "source": [
        "#Baseline models \n",
        "\n",
        "The fragments of code related to the baseline models (RF & LSVC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0uVzNGD0Grd"
      },
      "source": [
        "#code for loading additional features\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/readability_train.csv', sep=';', dtype='unicode', index_col=0)\n",
        "additional_train=df.values[1:]\n",
        "print(additional_train.shape)\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/readability_test.csv', sep=';', index_col=0)\n",
        "additional_test=df.values[1:]\n",
        "print(additional_test.shape)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "additional_train = scaler.fit_transform(additional_train)\n",
        "additional_test = scaler.transform(additional_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-1e8lEjfjj"
      },
      "source": [
        "PATH_TRAIN = '/content/drive/My Drive/children_and_adults/train/'\n",
        "PATH_TEST = '/content/drive/My Drive/children_and_adults/test/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CACFisUAO3C4",
        "outputId": "cb57967d-299e-43d4-804f-a5fa0b113018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#tfidf for abstracts\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "path_train='/content/drive/My Drive/children_and_adults/corpus/train'\n",
        "path_test='/content/drive/My Drive/children_and_adults/corpus/test'\n",
        "path_abstracts='/content/drive/My Drive/children_and_adults/corpus/abstracts'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqL3tBLJO45V"
      },
      "source": [
        "#adding abstracts\n",
        "\n",
        "files_previews=os.listdir(path_train)\n",
        "abstracts_train=[]\n",
        "\n",
        "for i,f in enumerate(files_previews):\n",
        "  with open(path_abstracts+'/'+f, 'r') as f42:\n",
        "    abstracts_train.append(f42.read())\n",
        "\n",
        "files_previews=os.listdir(path_test)\n",
        "abstracts_test=[]\n",
        "\n",
        "for i,f in enumerate(files_previews):\n",
        "  with open(path_abstracts+'/'+f, 'r') as f42:\n",
        "    abstracts_test.append(f42.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--eZFmnIO_sU",
        "outputId": "d16a350a-ccbb-4b46-8c5a-44e6aa9a6d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "#adding abstracts\n",
        "\n",
        "!pip install pymorphy2\n",
        "import pymorphy2, re\n",
        "ma = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) #deleting newlines and line-breaks\n",
        "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text) #deleting symbols \n",
        "    text = \" \".join(ma.parse(word)[0].normal_form for word in text.split())\n",
        "    text = ' '.join(word for word in text.split() if len(word)>3)\n",
        "    return text\n",
        "\n",
        "abstracts_train_cleaned=[]\n",
        "abstracts_test_cleaned=[]\n",
        "\n",
        "for ab in abstracts_train:\n",
        "  abstracts_train_cleaned.append(clean_text(ab))\n",
        "for ab in abstracts_test:\n",
        "  abstracts_test_cleaned.append(clean_text(ab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 6.6MB/s \n",
            "\u001b[?25hInstalling collected packages: dawg-python, pymorphy2-dicts, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPvX57Az5MV",
        "outputId": "9629ae9c-6b1b-410d-f22c-69df9554bb2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/train_texts_cleaned_short.pickle', 'rb') as handle:\n",
        "    train_texts=pickle.load(handle)\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/test_texts_cleaned_short.pickle', 'rb') as handle:\n",
        "    test_texts=pickle.load(handle)\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/train_labels2.pickle', 'rb') as handle:\n",
        "    train_labels=pickle.load(handle)\n",
        "with open('/content/drive/My Drive/children_and_adults/corpus/val_labels2.pickle', 'rb') as handle:\n",
        "    test_labels=pickle.load(handle)\n",
        "print(len(train_texts))\n",
        "print(len(test_texts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4492\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGhNCpMOH9CM",
        "outputId": "a72faf82-3190-40be-ca32-d6d27a20aab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "#adding all features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/rutez_train.csv', sep=';', index_col=0)\n",
        "X=df.values\n",
        "print(X.shape)\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/readability_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/general_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/gram_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/lexical_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/children_and_adults/age_rating_train.csv', sep=';', index_col=0)\n",
        "additional_train=df.values\n",
        "print(additional_train.shape)\n",
        "X=np.hstack((X,additional_train))\n",
        "\n",
        "print(X.shape)\n",
        "df=pd.DataFrame(X)\n",
        "df.to_csv('all_features_train.csv', sep=';', header=0, index=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4493, 6)\n",
            "(4493, 5)\n",
            "(4493, 12)\n",
            "(4493, 3)\n",
            "(4493, 26)\n",
            "(4493, 1)\n",
            "(4493, 53)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo2RwYb52Tqy",
        "outputId": "c2db9569-320b-40a3-9a9d-94a57abf3770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df=pd.read_csv('all_features.csv', sep=';')\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4492, 53)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPUxppfpkLG2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X = vectorizer.fit_transform(train_texts)\n",
        "X1 = vectorizer.transform(test_texts)\n",
        "#X = vectorizer.fit_transform(abstracts_train_cleaned)\n",
        "#X1 = vectorizer.transform(abstracts_test_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRUAbVv2JvZo",
        "outputId": "a4a056da-7b03-48ef-c355-c465b3be39d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4492, 2052)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZYStSDI8y8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X=np.hstack((X.todense(),additional_train))\n",
        "X1=np.hstack((X1.todense(),additional_test))\n",
        "#X=np.hstack((X,additional_train))\n",
        "#X1=np.hstack((X1,additional_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cBgPUTUKVxk"
      },
      "source": [
        "X=additional_train\n",
        "X1=additional_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5X5D_Xk-8B",
        "outputId": "97479553-0ae2-4269-89b1-3d3ab3532c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "#Random Forest\n",
        "best_res=0\n",
        "\n",
        "f1=0\n",
        "acc=0\n",
        "pres=0\n",
        "rec=0\n",
        "\n",
        "for rs in range(10):\n",
        "  RF=RandomForestClassifier(random_state=rs)\n",
        "  RF.fit(X, train_labels)\n",
        "  res=RF.predict(X1)\n",
        "  f1=f1_score(res, test_labels)*100\n",
        "  if f1>best_res:\n",
        "    best_res=f1\n",
        "    acc=accuracy_score(res, test_labels)*100\n",
        "    rec=recall_score(res, test_labels)*100\n",
        "    pres=precision_score(res, test_labels)*100\n",
        "\n",
        "print('Accuracy: %f' % acc)\n",
        "print('F1-score: %f' % best_res)\n",
        "print('Precision: %f' % pres)\n",
        "print('Recall: %f' % rec)\n",
        "\n",
        "#LInear SVC\n",
        "\n",
        "best_res=0\n",
        "\n",
        "f1=0\n",
        "acc=0\n",
        "pres=0\n",
        "rec=0\n",
        "\n",
        "for rs in range(10):\n",
        "  SVM=SVC(random_state=rs, tol=1e-5)\n",
        "  SVM.fit(X, train_labels)\n",
        "  res=SVM.predict(X1)\n",
        "  f1=f1_score(res, test_labels)*100\n",
        "  if f1>best_res:\n",
        "      best_res=f1\n",
        "      acc=accuracy_score(res, test_labels)*100\n",
        "      rec=recall_score(res, test_labels)*100\n",
        "      pres=precision_score(res, test_labels)*100\n",
        "\n",
        "print('Accuracy: %f' % acc)\n",
        "print('F1-score: %f' % best_res)\n",
        "print('Precision: %f' % pres)\n",
        "print('Recall: %f' % rec)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(.95)\n",
        "try:\n",
        "  X = pca.fit_transform(X.todense())\n",
        "  X1 = pca.transform(X1.todense())\n",
        "except:\n",
        "  X = pca.fit_transform(X)\n",
        "  X1 = pca.transform(X1) \n",
        "len(X[0])\n",
        "\n",
        "best_res=0\n",
        "\n",
        "f1=0\n",
        "acc=0\n",
        "pres=0\n",
        "rec=0\n",
        "\n",
        "for rs in range(10):\n",
        "  SVM=SVC(random_state=rs, tol=1e-5)\n",
        "  SVM.fit(X, train_labels)\n",
        "  res=SVM.predict(X1)\n",
        "  f1=f1_score(res, test_labels)*100\n",
        "  if f1>best_res:\n",
        "      best_res=f1\n",
        "      acc=accuracy_score(res, test_labels)*100\n",
        "      rec=recall_score(res, test_labels)*100\n",
        "      pres=precision_score(res, test_labels)*100\n",
        "\n",
        "print('Accuracy: %f' % acc)\n",
        "print('F1-score: %f' % best_res)\n",
        "print('Precision: %f' % pres)\n",
        "print('Recall: %f' % rec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.200000\n",
            "F1-score: 87.572816\n",
            "Precision: 90.200000\n",
            "Recall: 85.094340\n",
            "Accuracy: 87.400000\n",
            "F1-score: 87.450199\n",
            "Precision: 87.800000\n",
            "Recall: 87.103175\n",
            "Accuracy: 87.900000\n",
            "F1-score: 88.007929\n",
            "Precision: 88.800000\n",
            "Recall: 87.229862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn0IneEN9uz_",
        "outputId": "56fc6c18-136f-484d-f304-17793e9e92d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(train_texts)\n",
        "X1 = vectorizer.transform(test_texts)\n",
        "\n",
        "#Random Forest\n",
        "\n",
        "RF=RandomForestClassifier(random_state=0)\n",
        "RF.fit(X, train_labels)\n",
        "res=RF.predict(X1)\n",
        "print('Accuracy: %f' % (accuracy_score(res, test_labels)*100))\n",
        "print('F1-score: %f' % (f1_score(res, test_labels)*100))\n",
        "print('Precision: %f' % (precision_score(res, test_labels)*100))\n",
        "print('Recall: %f' % (recall_score(res, test_labels)*100))\n",
        "\n",
        "#LInear SVC\n",
        "\n",
        "SVM=LinearSVC(random_state=0, tol=1e-5)\n",
        "SVM.fit(X, train_labels)\n",
        "res=SVM.predict(X1)\n",
        "print('Accuracy: %f' % (accuracy_score(res, test_labels)*100))\n",
        "print('F1-score: %f' % (f1_score(res, test_labels)*100))\n",
        "print('Precision: %f' % (precision_score(res, test_labels)*100))\n",
        "print('Recall: %f' % (recall_score(res, test_labels)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.200000\n",
            "F1-score: 85.583942\n",
            "Precision: 93.800000\n",
            "Recall: 78.691275\n",
            "Accuracy: 85.600000\n",
            "F1-score: 86.206897\n",
            "Precision: 90.000000\n",
            "Recall: 82.720588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvyAeRC-mOAy"
      },
      "source": [
        "####readability\n",
        "\n",
        "import requests\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def average_sentence_length(text):\n",
        "  sentences=sent_tokenize(text)\n",
        "  l=0\n",
        "  for sent in sentences:\n",
        "    l+=len(word_tokenize(sent))\n",
        "  return l/len(sentences)\n",
        "\n",
        "def number_of_syllables(word):\n",
        "  count=0\n",
        "  word=word.lower()\n",
        "  for w in word:\n",
        "    if w=='а' or w=='ё' or w=='е' or w=='у' or w=='э' or w=='ю' or w=='и' or w=='я' or w=='ы' or w=='о':\n",
        "        count+=1\n",
        "  return count\n",
        "\n",
        "def average_syllabeles_word(text):\n",
        "  words=word_tokenize(text)\n",
        "  l=0\n",
        "  for word in words:\n",
        "    l+=number_of_syllables(word)\n",
        "  return l/len(words)\n",
        "\n",
        "values=[]\n",
        "values.append(['index_fk', 'index_cl', 'index_dc', 'index_ari', 'index_SMOG'])\n",
        "\n",
        "for text in train_texts:\n",
        "  response = requests.post(\"http://api.plainrussian.ru/api/1.0/ru/measure/\", data={\"text\":text})\n",
        "  res=response.json()\n",
        "  values.append([res['indexes']['index_fk'], res['indexes']['index_cl'], res['indexes']['index_dc'], res['indexes']['index_ari'], res['indexes']['index_SMOG']])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxC2I7YQ3Bzj"
      },
      "source": [
        "df=pd.DataFrame(values)\n",
        "df[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R675Z7smns0l"
      },
      "source": [
        "df.to_csv('readability_train.csv', sep=';', inde)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdWXlLtM_cH"
      },
      "source": [
        "with open('/content/drive/My Drive/children_and_adults/5000lemma.al', 'r', encoding='cp1251') as handle:\n",
        "  content=handle.readlines()\n",
        "\n",
        "words=[]\n",
        "freq=[]\n",
        "for i in range(1,len(content)):\n",
        "  elems=content[i].split()\n",
        "  words.append(elems[2])\n",
        "  freq.append(elems[1])\n",
        "\n",
        "dictionary = dict(zip(words, freq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc65REdVT7GG",
        "outputId": "797741c9-b45d-4873-acb9-9374598bb14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "path_freq='/content/drive/My Drive/freqrnc2011.csv'\n",
        "df=pd.read_csv(path_freq, sep='\\t')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lemma</th>\n",
              "      <th>PoS</th>\n",
              "      <th>Freq(ipm)</th>\n",
              "      <th>R</th>\n",
              "      <th>D</th>\n",
              "      <th>Doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>а</td>\n",
              "      <td>conj</td>\n",
              "      <td>8198.0</td>\n",
              "      <td>100</td>\n",
              "      <td>97</td>\n",
              "      <td>32332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>а</td>\n",
              "      <td>intj</td>\n",
              "      <td>19.8</td>\n",
              "      <td>99</td>\n",
              "      <td>90</td>\n",
              "      <td>757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>а</td>\n",
              "      <td>part</td>\n",
              "      <td>6.1</td>\n",
              "      <td>59</td>\n",
              "      <td>79</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>а</td>\n",
              "      <td>s</td>\n",
              "      <td>2.7</td>\n",
              "      <td>59</td>\n",
              "      <td>85</td>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>аа</td>\n",
              "      <td>intj</td>\n",
              "      <td>1.5</td>\n",
              "      <td>47</td>\n",
              "      <td>80</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52133</th>\n",
              "      <td>ящерица</td>\n",
              "      <td>s</td>\n",
              "      <td>3.6</td>\n",
              "      <td>77</td>\n",
              "      <td>74</td>\n",
              "      <td>158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52134</th>\n",
              "      <td>ящерка</td>\n",
              "      <td>s</td>\n",
              "      <td>0.4</td>\n",
              "      <td>27</td>\n",
              "      <td>82</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52135</th>\n",
              "      <td>ящик</td>\n",
              "      <td>s</td>\n",
              "      <td>75.4</td>\n",
              "      <td>100</td>\n",
              "      <td>94</td>\n",
              "      <td>1810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52136</th>\n",
              "      <td>ящичек</td>\n",
              "      <td>s</td>\n",
              "      <td>3.4</td>\n",
              "      <td>80</td>\n",
              "      <td>89</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52137</th>\n",
              "      <td>ящур</td>\n",
              "      <td>s</td>\n",
              "      <td>0.5</td>\n",
              "      <td>23</td>\n",
              "      <td>62</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52138 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Lemma   PoS  Freq(ipm)    R   D    Doc\n",
              "0            а  conj     8198.0  100  97  32332\n",
              "1            а  intj       19.8   99  90    757\n",
              "2            а  part        6.1   59  79    128\n",
              "3            а     s        2.7   59  85    160\n",
              "4           аа  intj        1.5   47  80     68\n",
              "...        ...   ...        ...  ...  ..    ...\n",
              "52133  ящерица     s        3.6   77  74    158\n",
              "52134   ящерка     s        0.4   27  82     33\n",
              "52135     ящик     s       75.4  100  94   1810\n",
              "52136   ящичек     s        3.4   80  89    212\n",
              "52137     ящур     s        0.5   23  62     23\n",
              "\n",
              "[52138 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3POrPInWLkR"
      },
      "source": [
        "list_dict=df['Lemma'].values\n",
        "pos_dict=df['PoS'].values\n",
        "freq_dict=df['Freq(ipm)'].values\n",
        "r_dict=df['R'].values\n",
        "d_dict=df['D'].values\n",
        "doc_dict=df['Doc'].values\n",
        "\n",
        "values_lex=[]\n",
        "values_lex.append(['words_fr', 'words_r', 'words_d', 'words_doc', 'v_fr', 'v_r', 'v_d', 'v_doc',\\\n",
        "            's_fr', 's_r', 's_d', 's_doc', 'adv_fr', 'adv_r', 'adv_d', 'adv_doc',\\\n",
        "            'adj_fr', 'adj_r', 'adj_d', 'adj_doc', 'prop_fr', 'prop_r', 'prop_d', 'prop_doc', '5000_proc', '5000_freq' ])\n",
        "values_gram=[]\n",
        "values_gram.append(['count_v', 'count_s', 'count_adj'])\n",
        "\n",
        "list_dict=[w.lower() for w in list_dict]\n",
        "\n",
        "for t in train_texts:\n",
        "  words=t.split()\n",
        "\n",
        "  words_in_list=0\n",
        "  verbs_in_list=0\n",
        "  nouns_in_list=0\n",
        "  adv_in_list=0\n",
        "  adj_in_list=0\n",
        "  propn_in_list=0\n",
        "\n",
        "  words_freq=0\n",
        "  words_r=0\n",
        "  words_d=0\n",
        "  words_doc=0\n",
        "\n",
        "  v_freq=0\n",
        "  v_r=0\n",
        "  v_d=0\n",
        "  v_doc=0\n",
        "\n",
        "  adv_freq=0\n",
        "  adv_r=0\n",
        "  adv_d=0\n",
        "  adv_doc=0\n",
        "\n",
        "  s_freq=0\n",
        "  s_r=0\n",
        "  s_d=0\n",
        "  s_doc=0\n",
        "\n",
        "  a_freq=0\n",
        "  a_r=0\n",
        "  a_d=0\n",
        "  a_doc=0\n",
        "\n",
        "  p_freq=0\n",
        "  p_r=0\n",
        "  p_d=0\n",
        "  p_doc=0\n",
        "\n",
        "  number_words=len(words)\n",
        "  number_in_list=0\n",
        "  number_out_of_list=0\n",
        "  freq=0\n",
        "\n",
        "  for word in words:\n",
        "    try:\n",
        "      freq=dictionary[word]\n",
        "      number_in_list+=1\n",
        "    except:\n",
        "      number_out_of_list+=1\n",
        "\n",
        "    for i, item in enumerate(list_dict):\n",
        "      if word==item:\n",
        "        words_in_list+=1\n",
        "        words_freq+=freq_dict[i]\n",
        "        words_r+=r_dict[i]\n",
        "        words_d+=d_dict[i]\n",
        "        words_doc+=doc_dict[i]\n",
        "\n",
        "        if pos_dict[i]=='v':\n",
        "          verbs_in_list+=1\n",
        "          v_freq+=freq_dict[i]\n",
        "          v_r+=r_dict[i]\n",
        "          v_d+=d_dict[i]\n",
        "          v_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='adv' or pos_dict[i]=='advpro':\n",
        "          adv_in_list+=1\n",
        "          adv_freq+=freq_dict[i]\n",
        "          adv_r+=r_dict[i]\n",
        "          adv_d+=d_dict[i]\n",
        "          adv_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s':\n",
        "          nouns_in_list+=1\n",
        "          s_freq+=freq_dict[i]\n",
        "          s_r+=r_dict[i]\n",
        "          s_d+=d_dict[i]\n",
        "          s_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='a':\n",
        "          adj_in_list+=1\n",
        "          a_freq+=freq_dict[i]\n",
        "          a_r+=r_dict[i]\n",
        "          a_d+=d_dict[i]\n",
        "          a_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s.PROP':\n",
        "          propn_in_list+=1\n",
        "          p_freq+=freq_dict[i]\n",
        "          p_r+=r_dict[i]\n",
        "          p_d+=d_dict[i]\n",
        "          p_doc+=doc_dict[i]\n",
        "\n",
        "  new_line=[]\n",
        "  if words_in_list!=0:\n",
        "    new_line.append(words_freq/words_in_list)\n",
        "    new_line.append(words_r/words_in_list)\n",
        "    new_line.append(words_d/words_in_list)\n",
        "    new_line.append(words_doc/words_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if verbs_in_list!=0:\n",
        "    new_line.append(v_freq/verbs_in_list)\n",
        "    new_line.append(v_r/verbs_in_list)\n",
        "    new_line.append(v_d/verbs_in_list)\n",
        "    new_line.append(v_doc/verbs_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if nouns_in_list!=0:\n",
        "    new_line.append(s_freq/nouns_in_list)\n",
        "    new_line.append(s_r/nouns_in_list)\n",
        "    new_line.append(s_d/nouns_in_list)\n",
        "    new_line.append(s_doc/nouns_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if adv_in_list!=0:\n",
        "    new_line.append(adv_freq/adv_in_list)\n",
        "    new_line.append(adv_r/adv_in_list)\n",
        "    new_line.append(adv_d/adv_in_list)\n",
        "    new_line.append(adv_doc/adv_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if adj_in_list!=0:\n",
        "    new_line.append(a_freq/adj_in_list)\n",
        "    new_line.append(a_r/adj_in_list)\n",
        "    new_line.append(a_d/adj_in_list)\n",
        "    new_line.append(a_doc/adj_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if propn_in_list!=0:\n",
        "    new_line.append(p_freq/propn_in_list)\n",
        "    new_line.append(p_r/propn_in_list)\n",
        "    new_line.append(p_d/propn_in_list)\n",
        "    new_line.append(p_doc/propn_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_words!=0:\n",
        "    new_line.append(float(number_in_list)/number_words)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_in_list!=0:\n",
        "    new_line.append(float(freq)/number_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "\n",
        "  values_lex.append(new_line)\n",
        "  new_line=[float(verbs_in_list)/len(words), float(nouns_in_list)/len(words), float(adj_in_list)/len(words)]\n",
        "  values_gram.append(new_line)\n",
        "\n",
        "df=pd.DataFrame(values_lex)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/lexical_train.csv', sep=';')\n",
        "df=pd.DataFrame(values_gram)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/gram_train.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqPzEvNen0Y0"
      },
      "source": [
        "list_dict=df['Lemma'].values\n",
        "pos_dict=df['PoS'].values\n",
        "freq_dict=df['Freq(ipm)'].values\n",
        "r_dict=df['R'].values\n",
        "d_dict=df['D'].values\n",
        "doc_dict=df['Doc'].values\n",
        "\n",
        "values_lex=[]\n",
        "values_lex.append(['words_fr', 'words_r', 'words_d', 'words_doc', 'v_fr', 'v_r', 'v_d', 'v_doc',\\\n",
        "            's_fr', 's_r', 's_d', 's_doc', 'adv_fr', 'adv_r', 'adv_d', 'adv_doc',\\\n",
        "            'adj_fr', 'adj_r', 'adj_d', 'adj_doc', 'prop_fr', 'prop_r', 'prop_d', 'prop_doc', '5000_proc', '5000_freq' ])\n",
        "values_gram=[]\n",
        "values_gram.append(['count_v', 'count_s', 'count_adj'])\n",
        "\n",
        "list_dict=[w.lower() for w in list_dict]\n",
        "\n",
        "for t in test_texts:\n",
        "  words=t.split()\n",
        "\n",
        "  words_in_list=0\n",
        "  verbs_in_list=0\n",
        "  nouns_in_list=0\n",
        "  adv_in_list=0\n",
        "  adj_in_list=0\n",
        "  propn_in_list=0\n",
        "\n",
        "  words_freq=0\n",
        "  words_r=0\n",
        "  words_d=0\n",
        "  words_doc=0\n",
        "\n",
        "  v_freq=0\n",
        "  v_r=0\n",
        "  v_d=0\n",
        "  v_doc=0\n",
        "\n",
        "  adv_freq=0\n",
        "  adv_r=0\n",
        "  adv_d=0\n",
        "  adv_doc=0\n",
        "\n",
        "  s_freq=0\n",
        "  s_r=0\n",
        "  s_d=0\n",
        "  s_doc=0\n",
        "\n",
        "  a_freq=0\n",
        "  a_r=0\n",
        "  a_d=0\n",
        "  a_doc=0\n",
        "\n",
        "  p_freq=0\n",
        "  p_r=0\n",
        "  p_d=0\n",
        "  p_doc=0\n",
        "\n",
        "  number_words=len(words)\n",
        "  number_in_list=0\n",
        "  number_out_of_list=0\n",
        "  freq=0\n",
        "\n",
        "  for word in words:\n",
        "    try:\n",
        "      freq=dictionary[word]\n",
        "      number_in_list+=1\n",
        "    except:\n",
        "      number_out_of_list+=1\n",
        "\n",
        "    for i, item in enumerate(list_dict):\n",
        "      if word==item:\n",
        "        words_in_list+=1\n",
        "        words_freq+=freq_dict[i]\n",
        "        words_r+=r_dict[i]\n",
        "        words_d+=d_dict[i]\n",
        "        words_doc+=doc_dict[i]\n",
        "\n",
        "        if pos_dict[i]=='v':\n",
        "          verbs_in_list+=1\n",
        "          v_freq+=freq_dict[i]\n",
        "          v_r+=r_dict[i]\n",
        "          v_d+=d_dict[i]\n",
        "          v_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='adv' or pos_dict[i]=='advpro':\n",
        "          adv_in_list+=1\n",
        "          adv_freq+=freq_dict[i]\n",
        "          adv_r+=r_dict[i]\n",
        "          adv_d+=d_dict[i]\n",
        "          adv_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s':\n",
        "          nouns_in_list+=1\n",
        "          s_freq+=freq_dict[i]\n",
        "          s_r+=r_dict[i]\n",
        "          s_d+=d_dict[i]\n",
        "          s_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='a':\n",
        "          adj_in_list+=1\n",
        "          a_freq+=freq_dict[i]\n",
        "          a_r+=r_dict[i]\n",
        "          a_d+=d_dict[i]\n",
        "          a_doc+=doc_dict[i]\n",
        "        elif pos_dict[i]=='s.PROP':\n",
        "          propn_in_list+=1\n",
        "          p_freq+=freq_dict[i]\n",
        "          p_r+=r_dict[i]\n",
        "          p_d+=d_dict[i]\n",
        "          p_doc+=doc_dict[i]\n",
        "\n",
        "  new_line=[]\n",
        "  if words_in_list!=0:\n",
        "    new_line.append(words_freq/words_in_list)\n",
        "    new_line.append(words_r/words_in_list)\n",
        "    new_line.append(words_d/words_in_list)\n",
        "    new_line.append(words_doc/words_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if verbs_in_list!=0:\n",
        "    new_line.append(v_freq/verbs_in_list)\n",
        "    new_line.append(v_r/verbs_in_list)\n",
        "    new_line.append(v_d/verbs_in_list)\n",
        "    new_line.append(v_doc/verbs_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if nouns_in_list!=0:\n",
        "    new_line.append(s_freq/nouns_in_list)\n",
        "    new_line.append(s_r/nouns_in_list)\n",
        "    new_line.append(s_d/nouns_in_list)\n",
        "    new_line.append(s_doc/nouns_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if adv_in_list!=0:\n",
        "    new_line.append(adv_freq/adv_in_list)\n",
        "    new_line.append(adv_r/adv_in_list)\n",
        "    new_line.append(adv_d/adv_in_list)\n",
        "    new_line.append(adv_doc/adv_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if adj_in_list!=0:\n",
        "    new_line.append(a_freq/adj_in_list)\n",
        "    new_line.append(a_r/adj_in_list)\n",
        "    new_line.append(a_d/adj_in_list)\n",
        "    new_line.append(a_doc/adj_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "\n",
        "  if propn_in_list!=0:\n",
        "    new_line.append(p_freq/propn_in_list)\n",
        "    new_line.append(p_r/propn_in_list)\n",
        "    new_line.append(p_d/propn_in_list)\n",
        "    new_line.append(p_doc/propn_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_words!=0:\n",
        "    new_line.append(float(number_in_list)/number_words)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "  \n",
        "  if number_in_list!=0:\n",
        "    new_line.append(float(freq)/number_in_list)\n",
        "  else:\n",
        "    new_line.append(0)\n",
        "\n",
        "  values_lex.append(new_line)\n",
        "  new_line=[float(verbs_in_list)/len(words), float(nouns_in_list)/len(words), float(adj_in_list)/len(words)]\n",
        "  values_gram.append(new_line)\n",
        "\n",
        "df=pd.DataFrame(values_lex)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/lexical_test.csv', sep=';')\n",
        "df=pd.DataFrame(values_gram)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/gram_test.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTFJlFGiVL37",
        "outputId": "7ff6e205-42a5-497e-d948-8e95c0230be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i,p in enumerate(df['PoS'].values):\n",
        "  if p=='pr':\n",
        "    print(df['Lemma'].values[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "а-ля\n",
            "без\n",
            "безо\n",
            "благодаря\n",
            "близ\n",
            "в\n",
            "вблизи\n",
            "ввиду\n",
            "вглубь\n",
            "вдоль\n",
            "взамен\n",
            "включая\n",
            "вкруг\n",
            "вместо\n",
            "вне\n",
            "внизу\n",
            "внутри\n",
            "внутрь\n",
            "во\n",
            "возле\n",
            "вокруг\n",
            "вопреки\n",
            "вослед\n",
            "вперед\n",
            "впереди\n",
            "вроде\n",
            "вслед\n",
            "вследствие\n",
            "выше\n",
            "для\n",
            "до\n",
            "за\n",
            "заместо\n",
            "из\n",
            "из-за\n",
            "изнутри\n",
            "изо\n",
            "из-под\n",
            "исключая\n",
            "к\n",
            "касательно\n",
            "ко\n",
            "кроме\n",
            "кругом\n",
            "меж\n",
            "между\n",
            "мимо\n",
            "минус\n",
            "на\n",
            "навроде\n",
            "над\n",
            "надо\n",
            "накануне\n",
            "наперекор\n",
            "наподобие\n",
            "напротив\n",
            "насчет\n",
            "ниже\n",
            "о\n",
            "об\n",
            "обо\n",
            "около\n",
            "окрест\n",
            "окромя\n",
            "от\n",
            "относительно\n",
            "ото\n",
            "перед\n",
            "передо\n",
            "по\n",
            "поверх\n",
            "под\n",
            "подле\n",
            "подо\n",
            "подобно\n",
            "по-за\n",
            "позади\n",
            "помимо\n",
            "по-над\n",
            "поперек\n",
            "посередине\n",
            "посередь\n",
            "после\n",
            "посреди\n",
            "посредине\n",
            "посредством\n",
            "превыше\n",
            "пред\n",
            "предо\n",
            "прежде\n",
            "при\n",
            "про\n",
            "промеж\n",
            "против\n",
            "противу\n",
            "путем\n",
            "ради\n",
            "с\n",
            "сверх\n",
            "свыше\n",
            "середь\n",
            "сзади\n",
            "сквозь\n",
            "со\n",
            "согласно\n",
            "сообразно\n",
            "соответственно\n",
            "спустя\n",
            "среди\n",
            "средь\n",
            "сродни\n",
            "супротив\n",
            "типа\n",
            "у\n",
            "через\n",
            "чрез\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDRu_9RFF5V9"
      },
      "source": [
        "###ЧАСТОТНЫЙ СЛОВАРЬ\n",
        "\n",
        "values = []\n",
        "values.append([])\n",
        "\n",
        "for t in test_texts:\n",
        "  text_words=t.split()\n",
        "  number_words=len(text_words)\n",
        "  number_in_list=0\n",
        "  number_out_of_list=0\n",
        "  freq=0\n",
        "  for w in text_words:\n",
        "    try:\n",
        "      freq=dictionary[w]\n",
        "      number_in_list+=1\n",
        "    except:\n",
        "      number_out_of_list+=1\n",
        "\n",
        "  try:\n",
        "    values.append([float(number_in_list)/number_words, float(freq)/number_in_list])\n",
        "  except:\n",
        "    values.append([0, 0])\n",
        "df=pd.DataFrame(values)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/lexmin_test.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT7z2lNFF3NH",
        "outputId": "f0aa647c-7471-4bcb-f594-e1007bc38468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#age rating\n",
        "import os\n",
        "\n",
        "PATH_DESCRIPTIONS = '/content/drive/My Drive/children_and_adults/description.csv'\n",
        "df=pd.read_csv(PATH_DESCRIPTIONS, sep=';', header=None)\n",
        "\n",
        "dictionary = dict(zip(df[0], df[3]))\n",
        "\n",
        "PATH_TRAIN = '/content/drive/My Drive/children_and_adults/train/'\n",
        "PATH_TEST = '/content/drive/My Drive/children_and_adults/test/'\n",
        "\n",
        "files=os.listdir(PATH_TRAIN)\n",
        "print(len(files))\n",
        "ratings=[]\n",
        "ratings.append(['age_rating'])\n",
        "\n",
        "for f in files:\n",
        "  rating=dictionary[str(f)]\n",
        "  if rating==0:\n",
        "    rating.append([0])\n",
        "  elif rating==6:\n",
        "    ratings.append([0.25])\n",
        "  elif rating==12:\n",
        "    ratings.append([0.5])\n",
        "  elif rating==16:\n",
        "    ratings.append([0.75])\n",
        "  elif rating==18:\n",
        "    ratings.append([1])\n",
        "  else:\n",
        "    ratings.append([0.5])\n",
        "df=pd.DataFrame(ratings)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/age_rating_train.csv', sep=';')\n",
        "\n",
        "files=os.listdir(PATH_TEST)\n",
        "ratings=[]\n",
        "ratings.append(['age_rating'])\n",
        "\n",
        "for f in files:\n",
        "  rating=dictionary[str(f)]\n",
        "  if rating==0:\n",
        "    rating.append([0])\n",
        "  elif rating==6:\n",
        "    ratings.append([0.25])\n",
        "  elif rating==12:\n",
        "    ratings.append([0.5])\n",
        "  elif rating==16:\n",
        "    ratings.append([0.75])\n",
        "  elif rating==18:\n",
        "    ratings.append([1])\n",
        "  else:\n",
        "    ratings.append([0.5])\n",
        "df=pd.DataFrame(ratings)\n",
        "df.to_csv('/content/drive/My Drive/children_and_adults/age_rating_test.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4492\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}